{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, company1, company2, price_col, training_dataset_ratio=0.8, nrm=1, one_episode_num_step=30):\n",
    "        \n",
    "        self.company1 = company1\n",
    "        self.stock_price1 = None\n",
    "        self.stock_price1_train = None\n",
    "        self.stock_price1_test = None\n",
    "        \n",
    "        self.company2 = company2\n",
    "        self.stock_price2 = None\n",
    "        self.stock_price2_train = None\n",
    "        self.stock_price2_test = None\n",
    "        \n",
    "        self.price_col = price_col\n",
    "        self.stock_price_length = None\n",
    "        \n",
    "        self.training_dataset_ratio = training_dataset_ratio\n",
    "        self.nrm = nrm\n",
    "        \n",
    "        self.stock_price_final = []\n",
    "        self.local_current_step = None\n",
    "        self.global_current_step = None\n",
    "        self.purpose = None\n",
    "        self.old_prupoese = None\n",
    "        self.one_episode_num_step = one_episode_num_step\n",
    "        \n",
    "        # read two company's stock price\n",
    "        self.load_data()\n",
    "        \n",
    "        # check if two stock price have same date-index\n",
    "        \n",
    "        \n",
    "        # extract close price from two stock price and convert to numpy array\n",
    "        self.extract_close_price()\n",
    "        \n",
    "        # split two stock close price into training set and testing set\n",
    "        self.split_dataset()\n",
    "        \n",
    "    \n",
    "    # read two company's stock price\n",
    "    def load_data(self):\n",
    "        \n",
    "        try:\n",
    "            self.stock_price1 = pd.read_csv(self.company1 + \".csv\")\n",
    "        except:\n",
    "            raise Exception(\"Cannot load {}\".format(self.company1 + \".csv\"))\n",
    "            \n",
    "        try:\n",
    "            self.stock_price2 = pd.read_csv(self.company2 + \".csv\")\n",
    "        except:\n",
    "            raise Exception(\"Cannot load {}\".format(self.company2 + \".csv\"))\n",
    "            \n",
    "    \n",
    "    # extract close price from two stock price and convert to numpy array\n",
    "    def extract_close_price(self):\n",
    "        \n",
    "        try:\n",
    "            self.stock_price1 = self.stock_price1[self.price_col]\n",
    "            self.stock_price2 = self.stock_price2[self.price_col]\n",
    "        except:\n",
    "            raise Exception(\"Cannot extract stock price column: {}.\".format(self.price_col))\n",
    "            \n",
    "        \n",
    "        self.stock_price1 = self.stock_price1.values\n",
    "        self.stock_price2 = self.stock_price2.values\n",
    "        self.stock_price_length = len(self.stock_price1)\n",
    "        \n",
    "        \n",
    "    # split two stock close price into training set and testing set\n",
    "    def split_dataset(self):\n",
    "        \n",
    "        index = round(self.stock_price_length * self.training_dataset_ratio)\n",
    "        \n",
    "        self.stock_price1_train = self.stock_price1[:index]\n",
    "        self.stock_price1_test = self.stock_price1[index:]\n",
    "        \n",
    "        self.stock_price2_train = self.stock_price2[:index]\n",
    "        self.stock_price2_test = self.stock_price2[index:]\n",
    "        \n",
    "        print(\"===============Environment Info===============\")\n",
    "        print(\"Stock1: {}\".format(self.company1))\n",
    "        print(\"Stock2: {}\".format(self.company2))\n",
    "        print(\"Price Column: {}\".format(self.price_col))\n",
    "        print(\"Ngative Return Multiplier: {}\".format(self.nrm))\n",
    "        print(\"Total number of day for training: {}\".format(str(len(self.stock_price1_train))))\n",
    "        print(\"Total number of day for testing: {}\".format(str(len(self.stock_price1_test))))\n",
    "        print(\"==============================================\")\n",
    "        \n",
    "    \n",
    "    # reset environment: must specify purpose for training or tetsing\n",
    "    def reset(self, purpose):\n",
    "        \n",
    "        self.purpose = purpose\n",
    "        \n",
    "        if self.purpose != self.old_prupoese:\n",
    "            self.prepare_final_data()\n",
    "            self.old_prupoese = self.purpose\n",
    "            self.global_current_step = 0\n",
    "        \n",
    "        self.global_current_step += 1\n",
    "        self.local_current_step = 0\n",
    "        \n",
    "        \n",
    "        if self.global_current_step == len(self.stock_price_final)-self.one_episode_num_step+1:\n",
    "            self.global_current_step = 0\n",
    "            \n",
    "        return self.stock_price_final[self.global_current_step]\n",
    "            \n",
    "            \n",
    "    # prepare train data\n",
    "    def prepare_final_data(self):\n",
    "        \n",
    "        # spread of two stock\n",
    "        if self.purpose == \"train\":\n",
    "            spread = self.stock_price1_train - self.stock_price2_train\n",
    "        else:\n",
    "            spread = self.stock_price1_test - self.stock_price2_test\n",
    "        \n",
    "        \n",
    "        for idx, value in enumerate(spread[:-1]):\n",
    "            \n",
    "            one_step = np.empty(shape=(10))\n",
    "            one_step_idx = 0\n",
    "            \n",
    "            # current spread\n",
    "            current_spread = value\n",
    "            one_step[one_step_idx] = current_spread\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # daily return of spread\n",
    "            daily_return_spread = spread[idx+1] - current_spread\n",
    "            one_step[one_step_idx] = daily_return_spread\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 15 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-15<0) else (idx-15)\n",
    "                spread_mean_15_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_15_days = value\n",
    "            \n",
    "            one_step[one_step_idx] = spread_mean_15_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 15 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_15_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 10 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-10<0) else (idx-10)\n",
    "                spread_mean_10_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_10_days = value\n",
    "            one_step[one_step_idx] = spread_mean_10_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 10 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_10_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 7 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-7<0) else (idx-7)\n",
    "                spread_mean_7_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_7_days = value\n",
    "            one_step[one_step_idx] = spread_mean_7_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 7 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_7_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 5 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-5<0) else (idx-5)\n",
    "                spread_mean_5_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_5_days = value\n",
    "            one_step[one_step_idx] = spread_mean_5_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 5 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_5_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            self.stock_price_final.append(one_step)\n",
    "            \n",
    "    \n",
    "    # go next step: must provide action\n",
    "    def step(self, action):\n",
    "        \n",
    "        # new state\n",
    "        self.local_current_step += 1\n",
    "        step_idx = self.global_current_step + self.local_current_step\n",
    "        new_state = self.stock_price_final[step_idx]\n",
    "        \n",
    "        # reward\n",
    "        reward = action * self.stock_price_final[step_idx-1][1]\n",
    "        if self.purpose == \"train\":\n",
    "            reward = reward * self.nrm if reward < 0 else reward\n",
    "            \n",
    "        # is done\n",
    "        done = True if((self.local_current_step == self.one_episode_num_step-1) or (self.global_current_step+self.local_current_step==len(self.stock_price_final)-1)) else False\n",
    "        \n",
    "        return new_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, \n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 learning_rate,\n",
    "                 exploration_rate, \n",
    "                 exploration_decay,\n",
    "                 exploration_min, \n",
    "                 replay_buffer_size,\n",
    "                 batch_size, \n",
    "                 q_value_discounter\n",
    "                ):\n",
    "        \n",
    "        # state dimension\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # action dimension\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # exploration rate, decay and min for agent\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "        \n",
    "        # replay buffer size, batch size for agent\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # replay_buffer will store: state1, action1, reward1, state2, done1\n",
    "        self.replay_buffer = np.empty((self.replay_buffer_size, 10+1+1+10+1))\n",
    "        self.replay_buffer_counter = 0\n",
    "        \n",
    "        # discounter of max q value among actions \n",
    "        self.q_value_discounter = q_value_discounter\n",
    "        \n",
    "        # build a critic\n",
    "        self.critic = self.build_nn()\n",
    "        self.critic.summary()\n",
    "        \n",
    "    \n",
    "    # build a neural network as critic\n",
    "    def build_nn(self):\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.state_dim, ), name=\"InputLayer\")\n",
    "        hidden1 = keras.layers.Dense(20, activation=\"relu\", name=\"HiddenLayer1\")(inputs)\n",
    "        hidden2 = keras.layers.Dense(10, activation=\"relu\", name=\"HiddenLayer2\")(hidden1)\n",
    "        outputs = keras.layers.Dense(3, activation=\"linear\", name=\"OutputLayer\")(hidden2)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "\n",
    "    # smaple action given a state\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        explore = np.random.uniform(low=0.0, high=1.0)\n",
    "        \n",
    "        # explore\n",
    "        if explore < self.exploration_rate:\n",
    "            action_index = np.random.choice(self.action_dim)\n",
    "        \n",
    "        else:\n",
    "            state = np.reshape(state, (1, self.state_dim))\n",
    "            action = self.critic.predict(state)[0]\n",
    "            action_index = np.argmax(action)\n",
    "        \n",
    "        # action should be: 0, 1, -1\n",
    "        action_index -= 1\n",
    "        \n",
    "        return action_index\n",
    "        \n",
    "        \n",
    "    # store experience into replay buffer\n",
    "    def store_experience(self, state1, action1, reward1, state2, done1):\n",
    "        \n",
    "        experience = np.empty((10+1+1+10+1))\n",
    "        experience[0:10] = state1[:]\n",
    "        experience[10] = action1\n",
    "        experience[11] = reward1\n",
    "        experience[12:22] = state2[:]\n",
    "        experience[22] = int(done1)\n",
    "        \n",
    "        self.replay_buffer[self.replay_buffer_counter % self.replay_buffer_size][:] = experience\n",
    "        self.replay_buffer_counter += 1\n",
    "        \n",
    "        \n",
    "    # train critic with experience from replay buffer\n",
    "    def train(self):\n",
    "        \n",
    "        if self.replay_buffer_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # sample batch of experience from replay buffer\n",
    "        low_idx = 0\n",
    "        high_idx = min(self.replay_buffer_counter, self.replay_buffer_size)\n",
    "        mask = np.random.randint(low=low_idx, high=high_idx, size=self.batch_size)\n",
    "        batch_experience = self.replay_buffer[mask][:]\n",
    "        \n",
    "        # each experience: state1, action1, reward1, state2, done1\n",
    "        for experience in batch_experience:\n",
    "            \n",
    "            state1 = experience[0:10]\n",
    "            action1 = experience[10]\n",
    "            reward1 = experience[11]\n",
    "            state2 = experience[12:22]\n",
    "            done1 = experience[22]\n",
    "            \n",
    "            # if state2 is final state in its episode\n",
    "            if done1 == 1:\n",
    "                target_reward = reward1\n",
    "            \n",
    "            # if not\n",
    "            else:\n",
    "                state = np.reshape(state2, (1, self.state_dim))\n",
    "                max_action_value = np.max(self.critic.predict(state)[0])\n",
    "                target_reward = reward1 + self.q_value_discounter * max_action_value\n",
    "            \n",
    "            state = np.reshape(state1, (1, self.state_dim))\n",
    "            action_value = self.critic.predict(state)\n",
    "            action_value[0][int(action1+1)] = target_reward\n",
    "            \n",
    "            self.critic.fit(x=state, y=action_value, verbose=0)\n",
    "                \n",
    "        \n",
    "        # decrease exploraction rate\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate*self.exploration_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairTrading Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTrading:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # state dimension \n",
    "        state_dim = 10\n",
    "        \n",
    "        # action dimension\n",
    "        action_dim = 3\n",
    "        \n",
    "        # learning rate for agent\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        # exploration rate, decay and min for agent\n",
    "        exploration_rate = 1\n",
    "        exploration_decay = 0.995\n",
    "        exploration_min = 0.01\n",
    "        \n",
    "        # replay buffer size, batch size for agent\n",
    "        replay_buffer_size = 500\n",
    "        batch_size = 50\n",
    "        \n",
    "        # discounter of max q value among actions \n",
    "        q_value_discounter = 0.95\n",
    "        \n",
    "        # build agent\n",
    "        self.agent = Agent(state_dim=state_dim,\n",
    "                           action_dim=action_dim,\n",
    "                           learning_rate=learning_rate,\n",
    "                           exploration_rate=exploration_rate,\n",
    "                           exploration_decay=exploration_decay,\n",
    "                           exploration_min=exploration_min,\n",
    "                           replay_buffer_size=replay_buffer_size,\n",
    "                           batch_size=batch_size,\n",
    "                           q_value_discounter=q_value_discounter\n",
    "                           )\n",
    "        \n",
    "        # build environment\n",
    "        self.env = Environment(company1=\"AAPL\",\n",
    "                               company2=\"GOOG\", \n",
    "                               price_col=\"Close\",\n",
    "                               training_dataset_ratio=0.99,\n",
    "                               nrm=500,\n",
    "                               one_episode_num_step=40)\n",
    "        \n",
    "        # total episode to play\n",
    "        self.total_episode = 2000\n",
    "        \n",
    "        # store every episode's total reward\n",
    "        self.episode_total_reward = []\n",
    "        \n",
    "        \n",
    "    # start to play pair trading\n",
    "    def start(self):\n",
    "        \n",
    "        for episode in range(self.total_episode):\n",
    "            \n",
    "            # total rewardin each episode\n",
    "            total_reward = 0\n",
    "            \n",
    "            # a flag to indicate if the episode ends\n",
    "            done = False\n",
    "            \n",
    "            # get environment's initial state\n",
    "            state1 = self.env.reset(purpose=\"train\")\n",
    "            \n",
    "            while done is False:\n",
    "                \n",
    "                # agent samples an action given state\n",
    "                action1 = self.agent.sample_action(state1)\n",
    "                \n",
    "                # environemnt enters next state given action\n",
    "                state2, reward1, done1 = self.env.step(action1)\n",
    "                \n",
    "                # store this step (experience)\n",
    "                self.agent.store_experience(state1, action1, reward1, state2, done1)\n",
    "                \n",
    "                # update local variable\n",
    "                state1 = state2\n",
    "                done = done1\n",
    "                total_reward += reward1\n",
    "                \n",
    "            # when this episode ends...\n",
    "            print(\"#%.3d Episode's Reward: %.4d\" %(episode, total_reward))\n",
    "            self.episode_total_reward.append(total_reward)\n",
    "            self.agent.train()\n",
    "            \n",
    "    def plot_result(self):\n",
    "        \n",
    "        x = list(range(0, self.total_episode))\n",
    "        y = self.episode_total_reward\n",
    "        plt.plot(x, y, color=\"blue\", label=\"nrm={}\".format(str(self.env.nrm)))\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.title(\"DQN Agent's Performance on Pair Trading\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.savefig(\"{}.png\".format(str(self.env.nrm)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_trading_game = PairTrading()\n",
    "pair_trading_game.start()\n",
    "pair_trading_game.plot_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
