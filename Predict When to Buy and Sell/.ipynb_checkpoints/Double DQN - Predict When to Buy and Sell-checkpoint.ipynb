{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, company1, company2, price_col, training_dataset_ratio=0.8, nrm=1, one_episode_num_step=30):\n",
    "        \n",
    "        self.company1 = company1\n",
    "        self.stock_price1 = None\n",
    "        self.stock_price1_train = None\n",
    "        self.stock_price1_test = None\n",
    "        \n",
    "        self.company2 = company2\n",
    "        self.stock_price2 = None\n",
    "        self.stock_price2_train = None\n",
    "        self.stock_price2_test = None\n",
    "        \n",
    "        self.price_col = price_col\n",
    "        self.stock_price_length = None\n",
    "        \n",
    "        self.training_dataset_ratio = training_dataset_ratio\n",
    "        self.nrm = nrm\n",
    "        \n",
    "        self.stock_price_final = []\n",
    "        self.local_current_step = None\n",
    "        self.global_current_step = None\n",
    "        self.purpose = None\n",
    "        self.old_prupoese = None\n",
    "        self.one_episode_num_step = one_episode_num_step\n",
    "        \n",
    "        # read two company's stock price\n",
    "        self.load_data()\n",
    "        \n",
    "        # extract close price from two stock price and convert to numpy array\n",
    "        self.extract_close_price()\n",
    "        \n",
    "        # split two stock close price into training set and testing set\n",
    "        self.split_dataset()\n",
    "        \n",
    "        # record system's info\n",
    "        self.system_holding_stock1_unit = None\n",
    "        self.system_holding_stock1_avg_price = None\n",
    "        self.system_holding_stock1_wait2sell = False\n",
    "        self.system_holding_stock2_unit = None\n",
    "        self.system_holding_stock2_avg_price = None\n",
    "        self.system_holding_stock2_wait2sell = False\n",
    "        \n",
    "    \n",
    "    # read two company's stock price\n",
    "    def load_data(self):\n",
    "        \n",
    "        try:\n",
    "            self.stock_price1 = pd.read_csv(self.company1 + \".csv\")\n",
    "        except:\n",
    "            raise Exception(\"Cannot load {}\".format(self.company1 + \".csv\"))\n",
    "            \n",
    "        try:\n",
    "            self.stock_price2 = pd.read_csv(self.company2 + \".csv\")\n",
    "        except:\n",
    "            raise Exception(\"Cannot load {}\".format(self.company2 + \".csv\"))\n",
    "            \n",
    "    \n",
    "    # extract close price from two stock price and convert to numpy array\n",
    "    def extract_close_price(self):\n",
    "        \n",
    "        try:\n",
    "            self.stock_price1 = self.stock_price1[self.price_col]\n",
    "            self.stock_price2 = self.stock_price2[self.price_col]\n",
    "        except:\n",
    "            raise Exception(\"Cannot extract stock price column: {}.\".format(self.price_col))\n",
    "            \n",
    "        \n",
    "        self.stock_price1 = self.stock_price1.values\n",
    "        self.stock_price2 = self.stock_price2.values\n",
    "        self.stock_price_length = len(self.stock_price1)\n",
    "        \n",
    "        \n",
    "    # split two stock close price into training set and testing set\n",
    "    def split_dataset(self):\n",
    "        \n",
    "        index = round(self.stock_price_length * self.training_dataset_ratio)\n",
    "        \n",
    "        self.stock_price1_train = self.stock_price1[:index]\n",
    "        self.stock_price1_test = self.stock_price1[index:]\n",
    "        \n",
    "        self.stock_price2_train = self.stock_price2[:index]\n",
    "        self.stock_price2_test = self.stock_price2[index:]\n",
    "        \n",
    "        print(\"===============Environment Info===============\")\n",
    "        print(\"Stock1: {}\".format(self.company1))\n",
    "        print(\"Stock2: {}\".format(self.company2))\n",
    "        print(\"Price Column: {}\".format(self.price_col))\n",
    "        print(\"Ngative Return Multiplier: {}\".format(self.nrm))\n",
    "        print(\"Number of Days in One Episode: {}\".format(self.one_episode_num_step))\n",
    "        print(\"Total number of day for training: {}\".format(str(len(self.stock_price1_train))))\n",
    "        print(\"Total number of day for testing: {}\".format(str(len(self.stock_price1_test))))\n",
    "        print(\"==============================================\")\n",
    "        \n",
    "    \n",
    "    # reset environment: must specify purpose for training or tetsing\n",
    "    def reset(self, purpose):\n",
    "        \n",
    "        self.purpose = purpose\n",
    "        \n",
    "        if self.purpose != self.old_prupoese:\n",
    "            self.prepare_final_data()\n",
    "            self.old_prupoese = self.purpose\n",
    "            self.global_current_step = -1\n",
    "        \n",
    "        self.global_current_step += 1\n",
    "        self.local_current_step = 0\n",
    "        \n",
    "        self.system_holding_stock1_unit = 0\n",
    "        self.system_holding_stock1_avg_price = 0\n",
    "        self.system_holding_stock1_wait2sell = False\n",
    "        self.system_holding_stock2_unit = 0\n",
    "        self.system_holding_stock2_avg_price = 0\n",
    "        self.system_holding_stock2_wait2sell = False\n",
    "        \n",
    "        if self.global_current_step == len(self.stock_price_final)-self.one_episode_num_step+1:\n",
    "            self.global_current_step = 0\n",
    "            \n",
    "        '''\n",
    "        format of state environment should return:\n",
    "        state: [current stock1 price,\n",
    "                number of units of stock1 which system holding,\n",
    "                current stock2 price,\n",
    "                number of units of stock2 which system holding,\n",
    "                current spread,\n",
    "                spread return,\n",
    "                spread mean during past 15 days,\n",
    "                current spread / spread mean during past 15 days,\n",
    "                spread mean during past 10 days,\n",
    "                current spread / spread mean during past 10 days\n",
    "                spread mean during past 7 days\n",
    "                current spread / spread mean during past 7 days\n",
    "                spread mean during past 5 days\n",
    "                current spread / spread mean during past 5 days]\n",
    "        '''\n",
    "        if self.purpose == \"train\":\n",
    "            stock_price1 = self.stock_price1_train[self.global_current_step]\n",
    "            stock_price2 = self.stock_price2_train[self.global_current_step]\n",
    "        else:\n",
    "            stock_price1 = self.stock_price1_test[self.global_current_step]\n",
    "            stock_price2 = self.stock_price2_test[self.global_current_step]\n",
    "        \n",
    "        \n",
    "        additional_state = np.array([stock_price1, self.system_holding_stock1_unit, stock_price2, self.system_holding_stock2_unit])\n",
    "        original_state = self.stock_price_final[self.global_current_step]\n",
    "        \n",
    "        return np.insert(original_state, 0, additional_state)\n",
    "            \n",
    "            \n",
    "    # prepare train data\n",
    "    def prepare_final_data(self):\n",
    "        \n",
    "        # spread of two stock\n",
    "        if self.purpose == \"train\":\n",
    "            spread = self.stock_price1_train - self.stock_price2_train\n",
    "        else:\n",
    "            spread = self.stock_price1_test - self.stock_price2_test\n",
    "        \n",
    "        \n",
    "        for idx, value in enumerate(spread):\n",
    "            \n",
    "            one_step = np.empty(shape=(10))\n",
    "            one_step_idx = 0\n",
    "            \n",
    "            # current spread\n",
    "            current_spread = value\n",
    "            one_step[one_step_idx] = current_spread\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # daily return of spread\n",
    "            yesterday_spread = spread[idx-1] if idx-1 >= 0 else value\n",
    "            daily_return_spread = current_spread - yesterday_spread\n",
    "            one_step[one_step_idx] = daily_return_spread\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 15 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-15<0) else (idx-15)\n",
    "                spread_mean_15_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_15_days = value\n",
    "            \n",
    "            one_step[one_step_idx] = spread_mean_15_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 15 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_15_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 10 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-10<0) else (idx-10)\n",
    "                spread_mean_10_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_10_days = value\n",
    "            one_step[one_step_idx] = spread_mean_10_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 10 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_10_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 7 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-7<0) else (idx-7)\n",
    "                spread_mean_7_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_7_days = value\n",
    "            one_step[one_step_idx] = spread_mean_7_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 7 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_7_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 5 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-5<0) else (idx-5)\n",
    "                spread_mean_5_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_5_days = value\n",
    "            one_step[one_step_idx] = spread_mean_5_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 5 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_5_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            self.stock_price_final.append(one_step)\n",
    "            \n",
    "    # get next new state\n",
    "    def get_new_state(self, step_idx):\n",
    "        \n",
    "        '''\n",
    "        format of state environment should return:\n",
    "        state: [current stock1 price,\n",
    "                number of units of stock1 which system holding,\n",
    "                current stock2 price,\n",
    "                number of units of stock2 which system holding,\n",
    "                current spread,\n",
    "                spread return,\n",
    "                spread mean during past 15 days,\n",
    "                current spread / spread mean during past 15 days,\n",
    "                spread mean during past 10 days,\n",
    "                current spread / spread mean during past 10 days\n",
    "                spread mean during past 7 days\n",
    "                current spread / spread mean during past 7 days\n",
    "                spread mean during past 5 days\n",
    "                current spread / spread mean during past 5 days]\n",
    "        '''\n",
    "        \n",
    "        if self.purpose == \"train\":\n",
    "            stock_price1 = self.stock_price1_train[step_idx]\n",
    "            stock_price2 = self.stock_price2_train[step_idx]\n",
    "        else:\n",
    "            stock_price1 = self.stock_price1_test[step_idx]\n",
    "            stock_price2 = self.stock_price2_test[step_idx]\n",
    "        \n",
    "        original_new_state = self.stock_price_final[step_idx]\n",
    "        additional_new_state = np.array([stock_price1, self.system_holding_stock1_unit, stock_price2, self.system_holding_stock2_unit])\n",
    "        return np.insert(original_new_state, 0, additional_new_state)\n",
    "    \n",
    "    \n",
    "    # calculate reward given system's action\n",
    "    def calculate_reward(self, step_idx, action):\n",
    "        \n",
    "        '''\n",
    "        format of action environment should receive:\n",
    "        action: [current pattern,\n",
    "                 quantity for two stocks]\n",
    "        \n",
    "        current pattern: \n",
    "        type: integer\n",
    "        0 => buy stock1 and sell stock2\n",
    "        1 => sell stock1 and buy stock2\n",
    "        2 => no operation\n",
    "        \n",
    "        quantity for two stocks: \n",
    "        type: list\n",
    "        [0~10, 0~10] (11*11 combination) \n",
    "        '''\n",
    "        \n",
    "        if self.purpose == \"train\":\n",
    "            stock_price1 = self.stock_price1_train[step_idx]\n",
    "            stock_price2 = self.stock_price2_train[step_idx]\n",
    "        else:\n",
    "            stock_price1 = self.stock_price1_test[step_idx]\n",
    "            stock_price2 = self.stock_price2_test[step_idx]\n",
    "            \n",
    "            \n",
    "        pattern = action[0]\n",
    "        quantity = action[1]\n",
    "        stock1_quantity = quantity[0]\n",
    "        stock2_quantity = quantity[1]\n",
    "        \n",
    "        # buy stock1 and sell stock2\n",
    "        if pattern == 0:\n",
    "            \n",
    "            # process stock1\n",
    "            \n",
    "            # already buy some units of stock1\n",
    "            if self.system_holding_stock1_wait2sell is True:\n",
    "                self.system_holding_stock1_wait2sell = True\n",
    "                \n",
    "                if (self.system_holding_stock1_unit + stock1_quantity) == 0:\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                \n",
    "                \n",
    "                self.system_holding_stock1_unit += stock1_quantity\n",
    "                reward = 0\n",
    "            \n",
    "            # already sell some units of stock1\n",
    "            else:\n",
    "                q = min(self.system_holding_stock1_unit, stock1_quantity)\n",
    "                reward = (self.system_holding_stock1_avg_price - stock_price1)*q\n",
    "                self.system_holding_stock1_unit -= q\n",
    "                stock1_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock1_unit > 0:\n",
    "                    # self.system_holding_stock1_wait2sell remains False\n",
    "                    # self.system_holding_stock1_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock1_unit == 0:\n",
    "                    # self.system_holding_stock1_wait2sell remains False\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to buy more\n",
    "                    if stock1_quantity > 0:\n",
    "                        self.system_holding_stock1_wait2sell = True\n",
    "                        self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                        self.system_holding_stock1_unit += stock1_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock1 which system holds should not be negative.\")\n",
    "                    \n",
    "            # process stock2\n",
    "            \n",
    "            # already buy some units of stock2\n",
    "            if self.system_holding_stock2_wait2sell is True:\n",
    "                \n",
    "                q = min(self.system_holding_stock2_unit, stock2_quantity)\n",
    "                reward = (stock_price2 - self.system_holding_stock2_avg_price)*q\n",
    "                self.system_holding_stock2_unit -= q\n",
    "                stock2_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock2_unit > 0:\n",
    "                    # self.system_holding_stock2_wait2sell remains True\n",
    "                    # self.system_holding_stock2_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock2_unit == 0:\n",
    "                    self.system_holding_stock2_wait2sell = False\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to sell more\n",
    "                    if stock2_quantity > 0:\n",
    "                        self.system_holding_stock2_wait2sell = False\n",
    "                        self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                        self.system_holding_stock2_unit += stock2_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock2 which system holds should not be negative.\")\n",
    "                \n",
    "            # already sell some units of stock2\n",
    "            else:\n",
    "                self.system_holding_stock2_wait2sell = False\n",
    "                \n",
    "                if (self.system_holding_stock2_unit + stock2_quantity) == 0:\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                \n",
    "                self.system_holding_stock2_unit += stock2_quantity\n",
    "                reward = 0\n",
    "                \n",
    "            \n",
    "            \n",
    "        # sell stock1 and buy stock2\n",
    "        elif pattern == 1:\n",
    "            \n",
    "            # process stock1\n",
    "            \n",
    "            # already buy some units of stock1\n",
    "            if self.system_holding_stock1_wait2sell is True:\n",
    "                \n",
    "                q = min(self.system_holding_stock1_unit, stock1_quantity)\n",
    "                reward = (stock_price1 - self.system_holding_stock1_avg_price)*q\n",
    "                self.system_holding_stock1_unit -= q\n",
    "                stock1_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock1_unit > 0:\n",
    "                    # self.system_holding_stock1_wait2sell remains True\n",
    "                    # self.system_holding_stock1_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock1_unit == 0:\n",
    "                    self.system_holding_stock1_wait2sell = False\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to sell more\n",
    "                    if stock1_quantity > 0:\n",
    "                        self.system_holding_stock1_wait2sell = False\n",
    "                        self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                        self.system_holding_stock1_unit += stock1_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock1 which system holds should not be negative.\")\n",
    "            \n",
    "            # already sell some units of stock1\n",
    "            else:\n",
    "                self.system_holding_stock1_wait2sell = False\n",
    "                \n",
    "                if (self.system_holding_stock1_unit + stock1_quantity) == 0:\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                self.system_holding_stock1_unit += stock1_quantity\n",
    "                reward = 0\n",
    "                \n",
    "                \n",
    "            # process stock2\n",
    "            \n",
    "            # already buy some units of stock2\n",
    "            if self.system_holding_stock2_wait2sell is True:\n",
    "                self.system_holding_stock2_wait2sell = True\n",
    "                \n",
    "                if (self.system_holding_stock2_unit + stock2_quantity) == 0:\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                \n",
    "                self.system_holding_stock2_unit += stock2_quantity\n",
    "                reward = 0\n",
    "                \n",
    "            # already sell some units of stock2\n",
    "            else:\n",
    "                \n",
    "                q = min(self.system_holding_stock2_unit, stock2_quantity)\n",
    "                reward = (self.system_holding_stock2_avg_price - stock_price2)*q\n",
    "                self.system_holding_stock2_unit -= q\n",
    "                stock2_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock2_unit > 0:\n",
    "                    # self.system_holding_stock2_wait2sell remains False\n",
    "                    # self.system_holding_stock2_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock2_unit == 0:\n",
    "                    self.system_holding_stock2_wait2sell = False\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to buy more\n",
    "                    if stock2_quantity > 0:\n",
    "                        self.system_holding_stock2_wait2sell = True\n",
    "                        self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                        self.system_holding_stock2_unit += stock2_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock2 which system holds should not be negative.\")\n",
    "            \n",
    "            \n",
    "        # no operation\n",
    "        elif pattern == 2:\n",
    "            reward = 0\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No pattern match.\")\n",
    "            \n",
    "            \n",
    "        if self.purpose == \"train\":\n",
    "            reward = reward * self.nrm if reward < 0 else reward\n",
    "            \n",
    "        return reward\n",
    "        \n",
    "    \n",
    "    # go next step: must provide action\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.local_current_step += 1\n",
    "        step_idx = self.global_current_step + self.local_current_step\n",
    "        \n",
    "        # get new state\n",
    "        new_state = self.get_new_state(step_idx)\n",
    "        \n",
    "        # calculate reward given action\n",
    "        reward = self.calculate_reward(step_idx, action)\n",
    "            \n",
    "        # is done\n",
    "        done = True if((self.local_current_step == self.one_episode_num_step-1) or (step_idx==len(self.stock_price_final)-1)) else False\n",
    "        \n",
    "        return new_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pattern Agent's State (14) :**\n",
    " - current stock1 price\n",
    " - number of units of stock1 which system holding\n",
    " - current stock2 price\n",
    " - number of units of stock2 which system holding\n",
    " - current spread\n",
    " - spread return\n",
    " - spread mean during past 15 days\n",
    " - current spread / spread mean during past 15 days\n",
    " - spread mean during past 10 days\n",
    " - current spread / spread mean during past 10 days\n",
    " - spread mean during past 7 days\n",
    " - current spread / spread mean during past 7 days\n",
    " - spread mean during past 5 days\n",
    " - current spread / spread mean during past 5 days\n",
    " \n",
    "    \n",
    "- **Pattern Agent's Action (3) :**\n",
    " - buy stock1 and sell stock2\n",
    " - sell stock1 and buy stock1\n",
    " - no operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternAgent:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim, \n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 exploration_rate,\n",
    "                 exploration_decay,\n",
    "                 exploration_min,\n",
    "                 replay_buffer_size,\n",
    "                 batch_size):\n",
    "        \n",
    "        # input and output dimension\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # learning rate \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # discount q value\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # exploration\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "        \n",
    "        # replay buffer\n",
    "        # an experience: [state1(14), action1(1), reward1(1), state2(14), done(1)]\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = np.empty((self.replay_buffer_size, 14+1+1+14+1))\n",
    "        self.replay_buffer_counter = 0\n",
    "        \n",
    "        # double deep Q network\n",
    "        self.update_critic = self.build_nn(model_name=\"PatternAgent-UpdateCritic\")\n",
    "        self.update_critic.summary()\n",
    "        self.target_critic = self.build_nn(model_name=\"PatternAgent-TargetCritic\")\n",
    "        self.target_critic.summary()\n",
    "        \n",
    "    \n",
    "    # build neural network as model\n",
    "    def build_nn(self, model_name):\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.state_dim, ), name=\"InputLayer\")\n",
    "        hidden1 = keras.layers.Dense(units=24, activation=\"relu\", name=\"HiddenLayer1\")(inputs)\n",
    "        hidden2 = keras.layers.Dense(units=12, activation=\"relu\", name=\"HiddenLayer2\")(hidden1)\n",
    "        outputs = keras.layers.Dense(units=self.action_dim, activation=\"linear\", name=\"OutputLayer\")(hidden2)\n",
    "        \n",
    "        model = keras.models.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # sample action\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        # explore\n",
    "        if np.random.uniform(0, 1) < self.exploration_rate:\n",
    "            action_idx = np.random.choice(3, 1)\n",
    "        \n",
    "        else:\n",
    "            state = np.reshape(state, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state)[0]\n",
    "            action_idx = np.argmax(action_value)\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "    \n",
    "    # store experience\n",
    "    def store_experience(self, state1, action1, reward1, state2, done):\n",
    "        \n",
    "        experience = np.empty((14+1+1+14+1))\n",
    "        experience[0:14] = state1[:]\n",
    "        experience[14] = action1\n",
    "        experience[15] = reward1\n",
    "        experience[16:30] = state2[:]\n",
    "        experience[30] = int(done)\n",
    "        \n",
    "        self.replay_buffer[self.replay_buffer_counter % self.replay_buffer_size] = experience\n",
    "        self.replay_buffer_counter += 1\n",
    "        \n",
    "    \n",
    "    # copy update critic's weight to target critic\n",
    "    def set_target_critic_weight(self):\n",
    "        \n",
    "        self.target_critic.set_weights(self.update_critic.get_weights())\n",
    "        \n",
    "    \n",
    "    # train update critic\n",
    "    def train(self):\n",
    "        \n",
    "        if self.replay_buffer_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        mask = np.random.choice(a=min(self.replay_buffer_counter, self.replay_buffer_size), size=self.batch_size)\n",
    "        batch_experience = self.replay_buffer[mask][:]\n",
    "        \n",
    "        for experience in batch_experience:\n",
    "            \n",
    "            state1 = experience[0:14]\n",
    "            action1 = experience[14]\n",
    "            reward1 = experience[15]\n",
    "            state2 = experience[16:30]\n",
    "            done = experience[30]\n",
    "            \n",
    "            if done is True:\n",
    "                target_reward = reward1\n",
    "            \n",
    "            else:\n",
    "                # select action by update critic\n",
    "                state2 = np.reshape(state2, (1, self.state_dim))\n",
    "                action_value = self.update_critic.predict(state2)[0]\n",
    "                action_idx = np.argmax(action_value)\n",
    "                \n",
    "                # estimate q value by target critic\n",
    "                action_value = self.target_critic.predict(state2)[0]\n",
    "                q_value = action_value[action_idx]\n",
    "                \n",
    "                # calculate target reward\n",
    "                target_reward = reward1 + self.gamma*q_value\n",
    "                \n",
    "            \n",
    "            # fit update critic with revised action value\n",
    "            state1 = np.reshape(state1, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state1)[0]\n",
    "            action_value[int(action1)] = target_reward\n",
    "            action_value = np.reshape(action_value, (1, self.action_dim))\n",
    "            self.update_critic.fit(x=state1, y=action_value, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "        # exploration rate decay\n",
    "        self.exploration_rate = max(self.exploration_rate*self.exploration_decay, self.exploration_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantity Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quantity Agent's State (14+1) :**\n",
    " - current stock1 price\n",
    " - number of units of stock1 which system holding\n",
    " - current stock2 price\n",
    " - number of units of stock2 which system holding\n",
    " - current spread\n",
    " - spread return\n",
    " - spread mean during past 15 days\n",
    " - current spread / spread mean during past 15 days\n",
    " - spread mean during past 10 days\n",
    " - current spread / spread mean during past 10 days\n",
    " - spread mean during past 7 days\n",
    " - current spread / spread mean during past 7 days\n",
    " - spread mean during past 5 days\n",
    " - current spread / spread mean during past 5 days\n",
    " - current pattern\n",
    "\n",
    "\n",
    "\n",
    "- **Quantity Agent's Action (11*11) :**\n",
    " - [0, 0]\n",
    " - [0, 1]\n",
    " - [0, 2]\n",
    " - [0, 3]\n",
    " - ...\n",
    " - [1, 0]\n",
    " - [1, 2]\n",
    " - [1, 3]\n",
    " - ...\n",
    " - [9, 0]\n",
    " - [9, 1]\n",
    " - [9, 2]\n",
    " - [9, 3]\n",
    " - ...\n",
    " - [10, 7]\n",
    " - [10, 8]\n",
    " - [10, 9]\n",
    " - [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantityAgent:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim, \n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 exploration_rate,\n",
    "                 exploration_decay,\n",
    "                 exploration_min,\n",
    "                 replay_buffer_size,\n",
    "                 batch_size):\n",
    "        \n",
    "        \n",
    "        # input and output dimension\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # learning rate \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # discount q value\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # exploration\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "        \n",
    "        # replay buffer\n",
    "        # an experience: [state1(15), action1(1), reward1(1), state2(15), done(1)]\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = np.empty((self.replay_buffer_size, 15+1+1+15+1))\n",
    "        self.replay_buffer_counter = 0\n",
    "        \n",
    "        # double deep Q network\n",
    "        self.update_critic = self.build_nn(model_name=\"QuantityAgent-UpdateCritic\")\n",
    "        self.update_critic.summary()\n",
    "        self.target_critic = self.build_nn(model_name=\"QuantityAgent-TargetCritic\")\n",
    "        self.target_critic.summary()\n",
    "        \n",
    "        # action list: [[0,0], [0,1], ..., [10,10]]\n",
    "        self.action_list = self.get_action_list()\n",
    "        \n",
    "    \n",
    "    # build neural network as model\n",
    "    def build_nn(self, model_name):\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.state_dim, ), name=\"InputLayer\")\n",
    "        hidden1 = keras.layers.Dense(units=28, activation=\"relu\", name=\"HiddenLayer1\")(inputs)\n",
    "        hidden2 = keras.layers.Dense(units=14, activation=\"relu\", name=\"HiddenLayer2\")(hidden1)\n",
    "        outputs = keras.layers.Dense(units=self.action_dim, activation=\"linear\", name=\"OutputLayer\")(hidden2)\n",
    "        \n",
    "        model = keras.models.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model    \n",
    "    \n",
    "    \n",
    "    # generate action list\n",
    "    def get_action_list(self):\n",
    "        \n",
    "        final_list = []\n",
    "        \n",
    "        for i in range(0, 11):\n",
    "            for j in range(0, 11):\n",
    "                temp_list = [i, j]\n",
    "                final_list.append(temp_list)\n",
    "                \n",
    "        return final_list\n",
    "    \n",
    "    \n",
    "    # sample action\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        # explore\n",
    "        if np.random.uniform(0, 1) <= self.exploration_rate:\n",
    "            action_idx = np.random.choice(11*11, 1)\n",
    "        \n",
    "        else:\n",
    "            state = np.reshape(state, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state)[0]\n",
    "            action_idx = np.argmax(action_value)\n",
    "            \n",
    "        quantity_list = self.get_quantity_list(action_idx)\n",
    "        \n",
    "        return quantity_list\n",
    "    \n",
    "    \n",
    "    # translate action index to quantity list\n",
    "    def get_quantity_list(self, action_idx):\n",
    "        \n",
    "        return self.action_list[int(action_idx)]\n",
    "    \n",
    "    # translate quantity list to action index\n",
    "    def get_action_idx(self, quantity_list):\n",
    "\n",
    "        for idx, lst in enumerate(self.action_list):\n",
    "            if lst == quantity_list:\n",
    "                return idx\n",
    "        \n",
    "    \n",
    "    # store experience\n",
    "    def store_experience(self, state1, quantity_list, reward1, state2, done):\n",
    "        \n",
    "        experience = np.empty((15+1+1+15+1))\n",
    "        experience[0:15] = state1[:]\n",
    "        experience[15] = self.get_action_idx(quantity_list)\n",
    "        experience[16] = reward1\n",
    "        experience[17:32] = state2[:]\n",
    "        experience[32] = int(done)\n",
    "        \n",
    "        self.replay_buffer[self.replay_buffer_counter % self.replay_buffer_size] = experience\n",
    "        self.replay_buffer_counter += 1\n",
    "        \n",
    "        \n",
    "    # copy update critic's weight to target critic\n",
    "    def set_target_critic_weight(self):\n",
    "        \n",
    "        self.target_critic.set_weights(self.update_critic.get_weights())\n",
    "        \n",
    "    \n",
    "    # train update critic\n",
    "    def train(self):\n",
    "        \n",
    "        if self.replay_buffer_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        mask = np.random.choice(a=min(self.replay_buffer_counter, self.replay_buffer_size), size=self.batch_size)\n",
    "        batch_experience = self.replay_buffer[mask][:]\n",
    "        \n",
    "        for experience in batch_experience:\n",
    "            \n",
    "            state1 = experience[0:15]\n",
    "            action1 = experience[15]\n",
    "            reward1 = experience[16]\n",
    "            state2 = experience[17:32]\n",
    "            done = experience[32]\n",
    "            \n",
    "            if done is True:\n",
    "                target_reward = reward1\n",
    "            \n",
    "            else:\n",
    "                # select action by update critic\n",
    "                state2 = np.reshape(state2, (1, self.state_dim))\n",
    "                action_value = self.update_critic.predict(state2)[0]\n",
    "                action_idx = np.argmax(action_value)\n",
    "                \n",
    "                # estimate q value by target critic\n",
    "                action_value = self.target_critic.predict(state2)[0]\n",
    "                q_value = action_value[action_idx]\n",
    "                \n",
    "                # calculate target reward\n",
    "                target_reward = reward1 + self.gamma*q_value\n",
    "                \n",
    "            \n",
    "            # fit update critic with revised action value\n",
    "            state1 = np.reshape(state1, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state1)[0]\n",
    "            action_value[int(action1)] = target_reward\n",
    "            action_value = np.reshape(action_value, (1, self.action_dim))\n",
    "            self.update_critic.fit(x=state1, y=action_value, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "        # exploration rate decay\n",
    "        self.exploration_rate = max(self.exploration_rate*self.exploration_decay, self.exploration_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **System's Mission :** System should implement pair-trading on two stocks, 'AAPL' and 'GOOG', and gain profits from it.\n",
    "\n",
    "\n",
    "- **System Composition :** System consists of two agents, pattern agent who deciding the pattern and quantity agent who deciding the quantity of two stocks to buy and sell.\n",
    "\n",
    "- **System's State (14) :**\n",
    " - current stock1 price\n",
    " - number of units of stock1 which system holding\n",
    " - current stock2 price\n",
    " - number of units of stock2 which system holding\n",
    " - current spread\n",
    " - spread return\n",
    " - spread mean during past 15 days\n",
    " - current spread / spread mean during past 15 days\n",
    " - spread mean during past 10 days\n",
    " - current spread / spread mean during past 10 days\n",
    " - spread mean during past 7 days\n",
    " - current spread / spread mean during past 7 days\n",
    " - spread mean during past 5 days\n",
    " - current spread / spread mean during past 5 days\n",
    " \n",
    "- **System's Action (3 x (11 x 11)) :**\n",
    " - [Current Pattern, [Quantity1, Quantity2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class System:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, \n",
    "                 pattern_agent_state_dim,\n",
    "                 pattern_agent_action_dim,\n",
    "                 pattern_agent_learning_rate,\n",
    "                 pattern_agent_gamma,\n",
    "                 pattern_agent_exploration_rate,\n",
    "                 pattern_agent_exploration_decay,\n",
    "                 pattern_agent_exploration_min,\n",
    "                 pattern_agent_replay_buffer_size, \n",
    "                 pattern_agent_batch_size,\n",
    "                 quantity_agent_state_dim,\n",
    "                 quantity_agent_action_dim,\n",
    "                 quantity_agent_learning_rate,\n",
    "                 quantity_agent_gamma,\n",
    "                 quantity_agent_exploration_rate, \n",
    "                 quantity_agent_exploration_decay, \n",
    "                 quantity_agent_exploration_min, \n",
    "                 quantity_agent_replay_buffer_size, \n",
    "                 quantity_agent_batch_size):\n",
    "        \n",
    "        \n",
    "        # build pattern agent\n",
    "        self.pattern_agent = PatternAgent(state_dim=pattern_agent_state_dim,\n",
    "                                          action_dim=pattern_agent_action_dim,\n",
    "                                          learning_rate=pattern_agent_learning_rate,\n",
    "                                          gamma=pattern_agent_gamma,\n",
    "                                          exploration_rate=pattern_agent_exploration_rate,\n",
    "                                          exploration_decay=pattern_agent_exploration_decay,\n",
    "                                          exploration_min=pattern_agent_exploration_min,\n",
    "                                          replay_buffer_size=pattern_agent_replay_buffer_size,\n",
    "                                          batch_size=pattern_agent_batch_size\n",
    "                                         )\n",
    "        \n",
    "        # build quantity agent\n",
    "        self.quantity_agent = QuantityAgent(state_dim=quantity_agent_state_dim,\n",
    "                                           action_dim=quantity_agent_action_dim,\n",
    "                                           learning_rate=quantity_agent_learning_rate,\n",
    "                                           gamma=quantity_agent_gamma,\n",
    "                                           exploration_rate=quantity_agent_exploration_rate,\n",
    "                                           exploration_decay=quantity_agent_exploration_decay,\n",
    "                                           exploration_min=quantity_agent_exploration_min,\n",
    "                                           replay_buffer_size=quantity_agent_replay_buffer_size,\n",
    "                                           batch_size=quantity_agent_batch_size\n",
    "                                          )\n",
    "        \n",
    "    # sample an action\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        '''\n",
    "        Format and contents of state which system will receive:\n",
    "        state = [\n",
    "            current stock1 price\n",
    "            number of units of stock1 which system holding\n",
    "            current stock2 price\n",
    "            number of units of stock2 which system holding\n",
    "            current spread\n",
    "            spread return\n",
    "            spread mean during past 15 days\n",
    "            current spread / spread mean during past 15 days\n",
    "            spread mean during past 10 days\n",
    "            current spread / spread mean during past 10 days\n",
    "            spread mean during past 7 days\n",
    "            current spread / spread mean during past 7 days\n",
    "            spread mean during past 5 days\n",
    "            current spread / spread mean during past 5 days\n",
    "        ]\n",
    "        \n",
    "        Format and contents of action which system will return:\n",
    "        action = [\n",
    "            pattern,\n",
    "            [quantity1, quantity2]\n",
    "        ]\n",
    "        '''\n",
    "        \n",
    "        pattern_action = self.pattern_agent.sample_action(state)\n",
    "        state = np.append(state, pattern_action)\n",
    "        quantity_action = self.quantity_agent.sample_action(state)\n",
    "        action = [pattern_action, quantity_action]\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    # store experience\n",
    "    def store_experience(self, state1, action1, reward1, state2, done):\n",
    "        \n",
    "        self.pattern_agent.store_experience(state1, action1[0], reward1, state2, done)\n",
    "        self.quantity_agent.store_experience(np.append(state1, action1[0]), action1[1], reward1, np.append(state2, self.pattern_agent.sample_action(state2)), done)\n",
    "        \n",
    "        \n",
    "    # train system\n",
    "    def train(self):\n",
    "        \n",
    "        self.pattern_agent.set_target_critic_weight()\n",
    "        self.pattern_agent.train()\n",
    "        \n",
    "        self.quantity_agent.set_target_critic_weight()\n",
    "        self.quantity_agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair Trading Game Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTradingGame:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # necessary parameter for system\n",
    "        pattern_agent_state_dim = 14\n",
    "        pattern_agent_action_dim = 3\n",
    "        pattern_agent_learning_rate = 0.001\n",
    "        pattern_agent_gamma = 0.95\n",
    "        pattern_agent_exploration_rate = 0.98\n",
    "        pattern_agent_exploration_decay = 0.995\n",
    "        pattern_agent_exploration_min = 0.01\n",
    "        pattern_agent_replay_buffer_size = 600\n",
    "        pattern_agent_batch_size = 55\n",
    "        \n",
    "        quantity_agent_state_dim = 15\n",
    "        quantity_agent_action_dim = 11*11\n",
    "        quantity_agent_learning_rate = 0.001\n",
    "        quantity_agent_gamma = 0.95\n",
    "        quantity_agent_exploration_rate = 0.98\n",
    "        quantity_agent_exploration_decay = 0.995\n",
    "        quantity_agent_exploration_min = 0.01\n",
    "        quantity_agent_replay_buffer_size = 700\n",
    "        quantity_agent_batch_size = 80\n",
    "        \n",
    "        # build system\n",
    "        self.system = System(pattern_agent_state_dim=pattern_agent_state_dim,\n",
    "                             pattern_agent_action_dim=pattern_agent_action_dim,\n",
    "                             pattern_agent_learning_rate=pattern_agent_learning_rate,\n",
    "                             pattern_agent_gamma=pattern_agent_gamma,\n",
    "                             pattern_agent_exploration_rate=pattern_agent_exploration_rate,\n",
    "                             pattern_agent_exploration_decay=pattern_agent_exploration_decay,\n",
    "                             pattern_agent_exploration_min=pattern_agent_exploration_min,\n",
    "                             pattern_agent_replay_buffer_size=pattern_agent_replay_buffer_size,\n",
    "                             pattern_agent_batch_size=pattern_agent_batch_size,\n",
    "                             quantity_agent_state_dim=quantity_agent_state_dim,\n",
    "                             quantity_agent_action_dim=quantity_agent_action_dim,\n",
    "                             quantity_agent_learning_rate=quantity_agent_learning_rate,\n",
    "                             quantity_agent_gamma=quantity_agent_gamma,\n",
    "                             quantity_agent_exploration_rate=quantity_agent_exploration_rate,\n",
    "                             quantity_agent_exploration_decay=quantity_agent_exploration_decay,\n",
    "                             quantity_agent_exploration_min=quantity_agent_exploration_min,\n",
    "                             quantity_agent_replay_buffer_size=quantity_agent_replay_buffer_size,\n",
    "                             quantity_agent_batch_size=quantity_agent_batch_size\n",
    "                             )\n",
    "        \n",
    "        # necessary parameter for environment\n",
    "        company1 = \"AAPL\"\n",
    "        company2 = \"GOOG\"\n",
    "        price_col = \"Close\"\n",
    "        training_dataset_ratio = 0.75\n",
    "        self.nrm = 1\n",
    "        one_episode_num_step = 40\n",
    "        \n",
    "        # build environment\n",
    "        self.env = Environment(company1=company1,\n",
    "                               company2=company2,\n",
    "                               price_col=price_col,\n",
    "                               training_dataset_ratio=training_dataset_ratio,\n",
    "                               nrm=self.nrm,\n",
    "                               one_episode_num_step=one_episode_num_step)\n",
    "        \n",
    "        \n",
    "        # total training episode for system, and store total reward in each episode\n",
    "        self.total_training_episode = 2000\n",
    "        self.training_episode_reward = []\n",
    "        \n",
    "        \n",
    "    \n",
    "    # start training system\n",
    "    def start_training(self):\n",
    "        \n",
    "        for episode in range(self.total_training_episode):\n",
    "            \n",
    "            # a flag to indicate the end of episode\n",
    "            done = False\n",
    "            \n",
    "            # reset environement\n",
    "            state1 = self.env.reset(purpose=\"train\")\n",
    "            \n",
    "            # total reward in this episode\n",
    "            total_reward = 0\n",
    "            \n",
    "            # in an episode ...\n",
    "            while done is False:\n",
    "                \n",
    "                # system will generate an action given current state\n",
    "                action1 = self.system.sample_action(state1)\n",
    "                \n",
    "                # environment will generate info given current action\n",
    "                state2, reward1, done = self.env.step(action1)\n",
    "                \n",
    "                # store this step (experience) into replay buffer\n",
    "                self.system.store_experience(state1, action1, reward1, state2, done)\n",
    "                \n",
    "                # update variable\n",
    "                state1 = state2\n",
    "                total_reward += reward1\n",
    "                \n",
    "            # when an episode ends ...\n",
    "            print(\"#%.4d Episode's Total Reward: %.4d\" %(episode, total_reward))\n",
    "            self.training_episode_reward.append(total_reward)\n",
    "            self.system.train()\n",
    "            \n",
    "    \n",
    "    # show training result\n",
    "    def show_training_result(self):\n",
    "        \n",
    "        x = list(range(0, self.total_training_episode))\n",
    "        y = self.training_episode_reward\n",
    "        plt.plot(x, y, label=\"nrm = {}\".format(self.nrm))\n",
    "        \n",
    "        plt.title(\"Double DQN's Performance on Pair Trading\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PatternAgent-UpdateCritic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputLayer (InputLayer)      (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "HiddenLayer1 (Dense)         (None, 24)                360       \n",
      "_________________________________________________________________\n",
      "HiddenLayer2 (Dense)         (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 699\n",
      "Trainable params: 699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"PatternAgent-TargetCritic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputLayer (InputLayer)      (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "HiddenLayer1 (Dense)         (None, 24)                360       \n",
      "_________________________________________________________________\n",
      "HiddenLayer2 (Dense)         (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 699\n",
      "Trainable params: 699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"QuantityAgent-UpdateCritic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputLayer (InputLayer)      (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "HiddenLayer1 (Dense)         (None, 28)                448       \n",
      "_________________________________________________________________\n",
      "HiddenLayer2 (Dense)         (None, 14)                406       \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 121)               1815      \n",
      "=================================================================\n",
      "Total params: 2,669\n",
      "Trainable params: 2,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"QuantityAgent-TargetCritic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputLayer (InputLayer)      (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "HiddenLayer1 (Dense)         (None, 28)                448       \n",
      "_________________________________________________________________\n",
      "HiddenLayer2 (Dense)         (None, 14)                406       \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 121)               1815      \n",
      "=================================================================\n",
      "Total params: 2,669\n",
      "Trainable params: 2,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "===============Environment Info===============\n",
      "Stock1: AAPL\n",
      "Stock2: GOOG\n",
      "Price Column: Close\n",
      "Ngative Return Multiplier: 1\n",
      "Number of Days in One Episode: 40\n",
      "Total number of day for training: 944\n",
      "Total number of day for testing: 314\n",
      "==============================================\n",
      "#0000 Episode's Total Reward: 0220\n",
      "#0001 Episode's Total Reward: 0222\n",
      "#0002 Episode's Total Reward: -0530\n",
      "#0003 Episode's Total Reward: 0477\n",
      "#0004 Episode's Total Reward: -0690\n",
      "#0005 Episode's Total Reward: 0353\n",
      "#0006 Episode's Total Reward: 0351\n",
      "#0007 Episode's Total Reward: -0236\n",
      "#0008 Episode's Total Reward: -1254\n",
      "#0009 Episode's Total Reward: -0354\n",
      "#0010 Episode's Total Reward: -0266\n",
      "#0011 Episode's Total Reward: 0162\n",
      "#0012 Episode's Total Reward: -0491\n",
      "#0013 Episode's Total Reward: -0895\n",
      "#0014 Episode's Total Reward: -0454\n",
      "#0015 Episode's Total Reward: 0295\n",
      "#0016 Episode's Total Reward: -0115\n",
      "#0017 Episode's Total Reward: -0516\n",
      "#0018 Episode's Total Reward: 0664\n",
      "#0019 Episode's Total Reward: 0598\n",
      "#0020 Episode's Total Reward: -0481\n",
      "#0021 Episode's Total Reward: 0768\n",
      "#0022 Episode's Total Reward: -0502\n",
      "#0023 Episode's Total Reward: -0474\n",
      "#0024 Episode's Total Reward: -0016\n",
      "#0025 Episode's Total Reward: 0321\n",
      "#0026 Episode's Total Reward: 0478\n",
      "#0027 Episode's Total Reward: -0334\n",
      "#0028 Episode's Total Reward: 0002\n",
      "#0029 Episode's Total Reward: 0453\n",
      "#0030 Episode's Total Reward: -0003\n",
      "#0031 Episode's Total Reward: 0001\n",
      "#0032 Episode's Total Reward: 0244\n",
      "#0033 Episode's Total Reward: 0084\n",
      "#0034 Episode's Total Reward: 0385\n",
      "#0035 Episode's Total Reward: 0091\n",
      "#0036 Episode's Total Reward: 0007\n",
      "#0037 Episode's Total Reward: 0058\n",
      "#0038 Episode's Total Reward: 0143\n",
      "#0039 Episode's Total Reward: 0271\n",
      "#0040 Episode's Total Reward: -0421\n",
      "#0041 Episode's Total Reward: -0092\n",
      "#0042 Episode's Total Reward: -0496\n",
      "#0043 Episode's Total Reward: 0435\n",
      "#0044 Episode's Total Reward: -0241\n",
      "#0045 Episode's Total Reward: -0502\n",
      "#0046 Episode's Total Reward: 1038\n",
      "#0047 Episode's Total Reward: -0324\n",
      "#0048 Episode's Total Reward: -0006\n",
      "#0049 Episode's Total Reward: -0347\n",
      "#0050 Episode's Total Reward: 0108\n",
      "#0051 Episode's Total Reward: -0258\n",
      "#0052 Episode's Total Reward: -0378\n",
      "#0053 Episode's Total Reward: -0258\n",
      "#0054 Episode's Total Reward: -0026\n",
      "#0055 Episode's Total Reward: -0585\n",
      "#0056 Episode's Total Reward: 0004\n",
      "#0057 Episode's Total Reward: -0405\n",
      "#0058 Episode's Total Reward: 0151\n",
      "#0059 Episode's Total Reward: -0240\n",
      "#0060 Episode's Total Reward: -0652\n",
      "#0061 Episode's Total Reward: -0061\n",
      "#0062 Episode's Total Reward: -0030\n",
      "#0063 Episode's Total Reward: -0306\n",
      "#0064 Episode's Total Reward: -0340\n",
      "#0065 Episode's Total Reward: -0089\n",
      "#0066 Episode's Total Reward: 0045\n",
      "#0067 Episode's Total Reward: -0056\n",
      "#0068 Episode's Total Reward: 0323\n",
      "#0069 Episode's Total Reward: -0300\n",
      "#0070 Episode's Total Reward: -0323\n",
      "#0071 Episode's Total Reward: -0058\n",
      "#0072 Episode's Total Reward: -0227\n",
      "#0073 Episode's Total Reward: -0638\n",
      "#0074 Episode's Total Reward: 0082\n",
      "#0075 Episode's Total Reward: -0225\n",
      "#0076 Episode's Total Reward: -0617\n",
      "#0077 Episode's Total Reward: -0496\n",
      "#0078 Episode's Total Reward: -0342\n",
      "#0079 Episode's Total Reward: 0452\n",
      "#0080 Episode's Total Reward: -0378\n",
      "#0081 Episode's Total Reward: -0398\n",
      "#0082 Episode's Total Reward: -0105\n",
      "#0083 Episode's Total Reward: -0213\n",
      "#0084 Episode's Total Reward: -0235\n",
      "#0085 Episode's Total Reward: -0536\n",
      "#0086 Episode's Total Reward: -0914\n",
      "#0087 Episode's Total Reward: -0634\n",
      "#0088 Episode's Total Reward: -0574\n",
      "#0089 Episode's Total Reward: -0309\n",
      "#0090 Episode's Total Reward: -0336\n",
      "#0091 Episode's Total Reward: -0678\n",
      "#0092 Episode's Total Reward: -0174\n",
      "#0093 Episode's Total Reward: -0331\n",
      "#0094 Episode's Total Reward: -0221\n",
      "#0095 Episode's Total Reward: -0267\n",
      "#0096 Episode's Total Reward: 0207\n",
      "#0097 Episode's Total Reward: -0501\n",
      "#0098 Episode's Total Reward: -0048\n",
      "#0099 Episode's Total Reward: -0310\n",
      "#0100 Episode's Total Reward: -0132\n",
      "#0101 Episode's Total Reward: 0135\n",
      "#0102 Episode's Total Reward: -0184\n",
      "#0103 Episode's Total Reward: -0263\n",
      "#0104 Episode's Total Reward: -0212\n",
      "#0105 Episode's Total Reward: 0041\n",
      "#0106 Episode's Total Reward: -0083\n",
      "#0107 Episode's Total Reward: -0027\n",
      "#0108 Episode's Total Reward: 0307\n",
      "#0109 Episode's Total Reward: 0003\n",
      "#0110 Episode's Total Reward: 0670\n",
      "#0111 Episode's Total Reward: -0089\n",
      "#0112 Episode's Total Reward: -0237\n",
      "#0113 Episode's Total Reward: 0018\n",
      "#0114 Episode's Total Reward: -0246\n",
      "#0115 Episode's Total Reward: -0176\n",
      "#0116 Episode's Total Reward: -0081\n",
      "#0117 Episode's Total Reward: 0655\n",
      "#0118 Episode's Total Reward: 0051\n",
      "#0119 Episode's Total Reward: 0262\n",
      "#0120 Episode's Total Reward: 0197\n",
      "#0121 Episode's Total Reward: -0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0122 Episode's Total Reward: -0106\n",
      "#0123 Episode's Total Reward: 0046\n",
      "#0124 Episode's Total Reward: 0000\n",
      "#0125 Episode's Total Reward: 0660\n",
      "#0126 Episode's Total Reward: -0098\n",
      "#0127 Episode's Total Reward: 0050\n",
      "#0128 Episode's Total Reward: -0159\n",
      "#0129 Episode's Total Reward: 0268\n",
      "#0130 Episode's Total Reward: 0038\n",
      "#0131 Episode's Total Reward: -0094\n",
      "#0132 Episode's Total Reward: 0597\n",
      "#0133 Episode's Total Reward: 0587\n",
      "#0134 Episode's Total Reward: 0037\n",
      "#0135 Episode's Total Reward: -0147\n",
      "#0136 Episode's Total Reward: -0733\n",
      "#0137 Episode's Total Reward: -0405\n",
      "#0138 Episode's Total Reward: -0734\n",
      "#0139 Episode's Total Reward: -0170\n",
      "#0140 Episode's Total Reward: -0525\n",
      "#0141 Episode's Total Reward: 0131\n",
      "#0142 Episode's Total Reward: -0470\n",
      "#0143 Episode's Total Reward: -0084\n",
      "#0144 Episode's Total Reward: 0218\n",
      "#0145 Episode's Total Reward: -0158\n",
      "#0146 Episode's Total Reward: 0014\n",
      "#0147 Episode's Total Reward: -0342\n",
      "#0148 Episode's Total Reward: 0587\n",
      "#0149 Episode's Total Reward: -0618\n",
      "#0150 Episode's Total Reward: -0686\n",
      "#0151 Episode's Total Reward: -0373\n",
      "#0152 Episode's Total Reward: -0241\n",
      "#0153 Episode's Total Reward: -0003\n",
      "#0154 Episode's Total Reward: 0000\n",
      "#0155 Episode's Total Reward: -0342\n",
      "#0156 Episode's Total Reward: 0346\n",
      "#0157 Episode's Total Reward: -0102\n",
      "#0158 Episode's Total Reward: -0288\n",
      "#0159 Episode's Total Reward: -0023\n",
      "#0160 Episode's Total Reward: -0232\n",
      "#0161 Episode's Total Reward: -0281\n",
      "#0162 Episode's Total Reward: 0307\n",
      "#0163 Episode's Total Reward: 0245\n",
      "#0164 Episode's Total Reward: 0160\n",
      "#0165 Episode's Total Reward: -0197\n",
      "#0166 Episode's Total Reward: 0111\n",
      "#0167 Episode's Total Reward: 0238\n",
      "#0168 Episode's Total Reward: 0333\n",
      "#0169 Episode's Total Reward: -0003\n",
      "#0170 Episode's Total Reward: 0251\n",
      "#0171 Episode's Total Reward: 0101\n",
      "#0172 Episode's Total Reward: 0264\n",
      "#0173 Episode's Total Reward: -0134\n",
      "#0174 Episode's Total Reward: -0051\n",
      "#0175 Episode's Total Reward: 0037\n",
      "#0176 Episode's Total Reward: 0236\n",
      "#0177 Episode's Total Reward: 0266\n",
      "#0178 Episode's Total Reward: 0262\n",
      "#0179 Episode's Total Reward: 0166\n",
      "#0180 Episode's Total Reward: -0202\n",
      "#0181 Episode's Total Reward: 0561\n",
      "#0182 Episode's Total Reward: 0303\n",
      "#0183 Episode's Total Reward: -0232\n",
      "#0184 Episode's Total Reward: 0037\n",
      "#0185 Episode's Total Reward: 0000\n",
      "#0186 Episode's Total Reward: -0037\n",
      "#0187 Episode's Total Reward: 0232\n",
      "#0188 Episode's Total Reward: 0067\n",
      "#0189 Episode's Total Reward: 0244\n",
      "#0190 Episode's Total Reward: -0119\n",
      "#0191 Episode's Total Reward: -0068\n",
      "#0192 Episode's Total Reward: -0074\n",
      "#0193 Episode's Total Reward: -0026\n",
      "#0194 Episode's Total Reward: -0125\n",
      "#0195 Episode's Total Reward: -0051\n",
      "#0196 Episode's Total Reward: -0149\n",
      "#0197 Episode's Total Reward: 0011\n",
      "#0198 Episode's Total Reward: -0016\n",
      "#0199 Episode's Total Reward: -0325\n",
      "#0200 Episode's Total Reward: -0083\n",
      "#0201 Episode's Total Reward: 0173\n",
      "#0202 Episode's Total Reward: 0186\n",
      "#0203 Episode's Total Reward: -0084\n",
      "#0204 Episode's Total Reward: 0045\n",
      "#0205 Episode's Total Reward: 0154\n",
      "#0206 Episode's Total Reward: 0155\n",
      "#0207 Episode's Total Reward: 0481\n",
      "#0208 Episode's Total Reward: 0338\n",
      "#0209 Episode's Total Reward: 0105\n",
      "#0210 Episode's Total Reward: 0081\n",
      "#0211 Episode's Total Reward: 0000\n",
      "#0212 Episode's Total Reward: 0494\n",
      "#0213 Episode's Total Reward: 0047\n",
      "#0214 Episode's Total Reward: 0124\n",
      "#0215 Episode's Total Reward: 0204\n",
      "#0216 Episode's Total Reward: 0083\n",
      "#0217 Episode's Total Reward: -0313\n",
      "#0218 Episode's Total Reward: -0169\n",
      "#0219 Episode's Total Reward: -0246\n",
      "#0220 Episode's Total Reward: 0028\n",
      "#0221 Episode's Total Reward: -0021\n",
      "#0222 Episode's Total Reward: -0061\n",
      "#0223 Episode's Total Reward: -0147\n",
      "#0224 Episode's Total Reward: -0060\n",
      "#0225 Episode's Total Reward: -0063\n",
      "#0226 Episode's Total Reward: -0008\n",
      "#0227 Episode's Total Reward: -0034\n",
      "#0228 Episode's Total Reward: -0055\n",
      "#0229 Episode's Total Reward: -0164\n",
      "#0230 Episode's Total Reward: -0063\n",
      "#0231 Episode's Total Reward: 0113\n",
      "#0232 Episode's Total Reward: 0098\n",
      "#0233 Episode's Total Reward: -0178\n",
      "#0234 Episode's Total Reward: 0181\n",
      "#0235 Episode's Total Reward: 0000\n",
      "#0236 Episode's Total Reward: -0005\n",
      "#0237 Episode's Total Reward: 0142\n",
      "#0238 Episode's Total Reward: 0089\n",
      "#0239 Episode's Total Reward: 0007\n",
      "#0240 Episode's Total Reward: -0001\n",
      "#0241 Episode's Total Reward: -0087\n",
      "#0242 Episode's Total Reward: -0045\n",
      "#0243 Episode's Total Reward: -0187\n",
      "#0244 Episode's Total Reward: -0138\n",
      "#0245 Episode's Total Reward: -0044\n",
      "#0246 Episode's Total Reward: -0152\n",
      "#0247 Episode's Total Reward: 0275\n",
      "#0248 Episode's Total Reward: -1193\n",
      "#0249 Episode's Total Reward: 0099\n",
      "#0250 Episode's Total Reward: -0050\n",
      "#0251 Episode's Total Reward: 0084\n",
      "#0252 Episode's Total Reward: -0913\n",
      "#0253 Episode's Total Reward: -0345\n",
      "#0254 Episode's Total Reward: 0058\n",
      "#0255 Episode's Total Reward: -0119\n",
      "#0256 Episode's Total Reward: 0011\n",
      "#0257 Episode's Total Reward: 0759\n",
      "#0258 Episode's Total Reward: 0628\n",
      "#0259 Episode's Total Reward: -0484\n",
      "#0260 Episode's Total Reward: -1769\n",
      "#0261 Episode's Total Reward: -0378\n",
      "#0262 Episode's Total Reward: -0530\n",
      "#0263 Episode's Total Reward: -0595\n",
      "#0264 Episode's Total Reward: 0000\n",
      "#0265 Episode's Total Reward: -0324\n",
      "#0266 Episode's Total Reward: -0270\n",
      "#0267 Episode's Total Reward: -0748\n",
      "#0268 Episode's Total Reward: -0307\n",
      "#0269 Episode's Total Reward: -0265\n",
      "#0270 Episode's Total Reward: 0036\n",
      "#0271 Episode's Total Reward: -0113\n",
      "#0272 Episode's Total Reward: 0231\n",
      "#0273 Episode's Total Reward: 0566\n",
      "#0274 Episode's Total Reward: -0320\n",
      "#0275 Episode's Total Reward: -0018\n",
      "#0276 Episode's Total Reward: -0253\n",
      "#0277 Episode's Total Reward: -0098\n",
      "#0278 Episode's Total Reward: -0224\n",
      "#0279 Episode's Total Reward: -0005\n",
      "#0280 Episode's Total Reward: -0219\n",
      "#0281 Episode's Total Reward: -0139\n",
      "#0282 Episode's Total Reward: -0050\n",
      "#0283 Episode's Total Reward: 0011\n",
      "#0284 Episode's Total Reward: -0182\n",
      "#0285 Episode's Total Reward: -0102\n",
      "#0286 Episode's Total Reward: -0192\n",
      "#0287 Episode's Total Reward: -0186\n",
      "#0288 Episode's Total Reward: -0102\n",
      "#0289 Episode's Total Reward: -0197\n",
      "#0290 Episode's Total Reward: -0048\n",
      "#0291 Episode's Total Reward: -0131\n",
      "#0292 Episode's Total Reward: -0393\n",
      "#0293 Episode's Total Reward: -0178\n",
      "#0294 Episode's Total Reward: -0058\n",
      "#0295 Episode's Total Reward: -0196\n",
      "#0296 Episode's Total Reward: 0000\n",
      "#0297 Episode's Total Reward: -0045\n",
      "#0298 Episode's Total Reward: 0000\n",
      "#0299 Episode's Total Reward: -0047\n",
      "#0300 Episode's Total Reward: 0000\n",
      "#0301 Episode's Total Reward: -0460\n",
      "#0302 Episode's Total Reward: -0179\n",
      "#0303 Episode's Total Reward: -0303\n",
      "#0304 Episode's Total Reward: -0431\n",
      "#0305 Episode's Total Reward: -0465\n",
      "#0306 Episode's Total Reward: -0336\n",
      "#0307 Episode's Total Reward: -0278\n",
      "#0308 Episode's Total Reward: -0097\n",
      "#0309 Episode's Total Reward: -0763\n",
      "#0310 Episode's Total Reward: -0543\n",
      "#0311 Episode's Total Reward: -0343\n",
      "#0312 Episode's Total Reward: -0808\n",
      "#0313 Episode's Total Reward: 0000\n",
      "#0314 Episode's Total Reward: 0000\n",
      "#0315 Episode's Total Reward: -0365\n",
      "#0316 Episode's Total Reward: 0016\n",
      "#0317 Episode's Total Reward: -0378\n",
      "#0318 Episode's Total Reward: 0000\n",
      "#0319 Episode's Total Reward: -0128\n",
      "#0320 Episode's Total Reward: -0200\n",
      "#0321 Episode's Total Reward: -0238\n",
      "#0322 Episode's Total Reward: -0031\n",
      "#0323 Episode's Total Reward: 0013\n",
      "#0324 Episode's Total Reward: -0074\n",
      "#0325 Episode's Total Reward: -0253\n",
      "#0326 Episode's Total Reward: 0256\n",
      "#0327 Episode's Total Reward: -0157\n",
      "#0328 Episode's Total Reward: -0245\n",
      "#0329 Episode's Total Reward: 0000\n",
      "#0330 Episode's Total Reward: 0026\n",
      "#0331 Episode's Total Reward: 0095\n",
      "#0332 Episode's Total Reward: 0006\n",
      "#0333 Episode's Total Reward: -0068\n",
      "#0334 Episode's Total Reward: -0019\n",
      "#0335 Episode's Total Reward: 0019\n",
      "#0336 Episode's Total Reward: 0000\n",
      "#0337 Episode's Total Reward: -0020\n",
      "#0338 Episode's Total Reward: -0082\n",
      "#0339 Episode's Total Reward: 0013\n",
      "#0340 Episode's Total Reward: 0000\n",
      "#0341 Episode's Total Reward: 0000\n",
      "#0342 Episode's Total Reward: -0116\n",
      "#0343 Episode's Total Reward: 0000\n",
      "#0344 Episode's Total Reward: -0064\n",
      "#0345 Episode's Total Reward: -0191\n",
      "#0346 Episode's Total Reward: 0000\n",
      "#0347 Episode's Total Reward: 0023\n",
      "#0348 Episode's Total Reward: 0000\n",
      "#0349 Episode's Total Reward: 0000\n",
      "#0350 Episode's Total Reward: -0010\n",
      "#0351 Episode's Total Reward: 0000\n",
      "#0352 Episode's Total Reward: -0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0353 Episode's Total Reward: 0018\n",
      "#0354 Episode's Total Reward: -0019\n",
      "#0355 Episode's Total Reward: -0372\n",
      "#0356 Episode's Total Reward: -0072\n",
      "#0357 Episode's Total Reward: 0015\n",
      "#0358 Episode's Total Reward: 0065\n",
      "#0359 Episode's Total Reward: 0006\n",
      "#0360 Episode's Total Reward: 0021\n",
      "#0361 Episode's Total Reward: -1522\n",
      "#0362 Episode's Total Reward: -0933\n",
      "#0363 Episode's Total Reward: -0026\n",
      "#0364 Episode's Total Reward: -0064\n",
      "#0365 Episode's Total Reward: 0000\n",
      "#0366 Episode's Total Reward: -0147\n",
      "#0367 Episode's Total Reward: 0044\n",
      "#0368 Episode's Total Reward: -0134\n",
      "#0369 Episode's Total Reward: -0064\n",
      "#0370 Episode's Total Reward: 0011\n",
      "#0371 Episode's Total Reward: 0000\n",
      "#0372 Episode's Total Reward: 0247\n",
      "#0373 Episode's Total Reward: -0010\n",
      "#0374 Episode's Total Reward: -0712\n",
      "#0375 Episode's Total Reward: -0004\n",
      "#0376 Episode's Total Reward: -1897\n",
      "#0377 Episode's Total Reward: -0251\n",
      "#0378 Episode's Total Reward: -0417\n",
      "#0379 Episode's Total Reward: -0390\n",
      "#0380 Episode's Total Reward: 0000\n",
      "#0381 Episode's Total Reward: -0298\n",
      "#0382 Episode's Total Reward: -0408\n",
      "#0383 Episode's Total Reward: 0000\n",
      "#0384 Episode's Total Reward: -1005\n",
      "#0385 Episode's Total Reward: -0412\n",
      "#0386 Episode's Total Reward: -0153\n",
      "#0387 Episode's Total Reward: 0043\n",
      "#0388 Episode's Total Reward: -0496\n",
      "#0389 Episode's Total Reward: -0217\n",
      "#0390 Episode's Total Reward: -0129\n",
      "#0391 Episode's Total Reward: -0158\n",
      "#0392 Episode's Total Reward: 0063\n",
      "#0393 Episode's Total Reward: -0741\n",
      "#0394 Episode's Total Reward: 0000\n",
      "#0395 Episode's Total Reward: 0000\n",
      "#0396 Episode's Total Reward: 0000\n",
      "#0397 Episode's Total Reward: 0039\n",
      "#0398 Episode's Total Reward: -0037\n",
      "#0399 Episode's Total Reward: 0000\n",
      "#0400 Episode's Total Reward: 0003\n",
      "#0401 Episode's Total Reward: -0045\n",
      "#0402 Episode's Total Reward: -0018\n",
      "#0403 Episode's Total Reward: -0120\n",
      "#0404 Episode's Total Reward: 0000\n",
      "#0405 Episode's Total Reward: 0000\n",
      "#0406 Episode's Total Reward: 0195\n",
      "#0407 Episode's Total Reward: 0093\n",
      "#0408 Episode's Total Reward: 0546\n",
      "#0409 Episode's Total Reward: 0540\n",
      "#0410 Episode's Total Reward: 0147\n",
      "#0411 Episode's Total Reward: 0365\n",
      "#0412 Episode's Total Reward: -0180\n",
      "#0413 Episode's Total Reward: 0287\n",
      "#0414 Episode's Total Reward: 0255\n",
      "#0415 Episode's Total Reward: -0267\n",
      "#0416 Episode's Total Reward: -0147\n",
      "#0417 Episode's Total Reward: 0298\n",
      "#0418 Episode's Total Reward: 0329\n",
      "#0419 Episode's Total Reward: -0028\n",
      "#0420 Episode's Total Reward: -0047\n",
      "#0421 Episode's Total Reward: 2619\n",
      "#0422 Episode's Total Reward: 0000\n",
      "#0423 Episode's Total Reward: -0208\n",
      "#0424 Episode's Total Reward: 0000\n"
     ]
    }
   ],
   "source": [
    "pair_trading_game = PairTradingGame()\n",
    "\n",
    "pair_trading_game.start_training()\n",
    "pair_trading_game.show_training_result()\n",
    "\n",
    "# pair_trading_game.start_testing()\n",
    "# pair_trading_game.show_testing_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
